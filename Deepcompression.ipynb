{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You are asked to complete the following files:\n",
    "* **pruned_layers.py**, which contains the pruning of DNNs to reduce the storage of insignificant weight parameters with 2 methods: pruning by percentage and prune by standara deviation.\n",
    "* **train_util.py**, which includes the training process of DNNs with pruned connections.\n",
    "* **quantize.py**, which applies the quantization (weight sharing) part on the DNN to reduce the storage of weight parameters.\n",
    "* **huffman_coding.py**, which applies the Huffman coding onto the weight of DNNs to further compress the weight size.\n",
    "\n",
    "You are asked to submit the following files:\n",
    "* **net_before_pruning.pt**, which is the weight parameters before applying pruning on DNN weight parameters.\n",
    "* **net_after_pruning.pt**, which is the weight paramters after applying pruning on DNN weight parameters.\n",
    "* **net_after_quantization.pt**, which is the weight parameters after applying quantization (weight sharing) on DNN weight parameters.\n",
    "* **codebook_vgg16.npy**, which is the quantization codebook of each layer after applying quantization (weight sharing).\n",
    "* **huffman_encoding.npy**, which is the encoding map of each item within the quantization codebook in the whole DNN architecture.\n",
    "* **huffman_freq.npy**, which is the frequency map of each item within the quantization codebook in the whole DNN. \n",
    "\n",
    "To ensure fair grading policy, we fix the choice of model to VGG16_half, which is a down-scaled version of VGG16 using a width multiplier of 0.5. You may check the implementation in **vgg16.py** for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from vgg16 import VGG16, VGG16_half\n",
    "from train_util import train, finetune_after_prune, test\n",
    "from quantize import quantize_whole_model\n",
    "from huffman_coding import huffman_coding\n",
    "from summary import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "from prune import prune\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-precision model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = VGG16_half()\n",
    "net = net.to(device)\n",
    "\n",
    "# Uncomment to load pretrained weights\n",
    "#net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "\n",
    "# Comment if you have loaded pretrained weights\n",
    "# Tune the hyperparameters here.\n",
    "train(net, epochs=10, batch_size=233, lr=0.00, reg=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----Summary before pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning & Finetune with pruned connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy before fine-tuning\n",
    "prune(net, method='std', q=45.0, s=0.75)\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load pretrained weights\n",
    "# net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "finetune_after_prune(net, epochs=50, batch_size=128, lr=0.001, reg=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----Summary After pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = quantize_whole_model(net, bits=5)\n",
    "np.save(\"codebook_vgg16.npy\", centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_map, encoding_map = huffman_coding(net, centers)\n",
    "np.save(\"huffman_encoding\", encoding_map)\n",
    "np.save(\"huffman_freq\", frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
